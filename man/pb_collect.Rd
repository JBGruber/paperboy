% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/collect.R
\name{pb_collect}
\alias{pb_collect}
\title{Collect data from supplied URLs}
\usage{
pb_collect(
  urls,
  collect_rss = TRUE,
  timeout = 30,
  ignore_fails = FALSE,
  connections = 100L,
  host_con = 6L,
  use_cookies = FALSE,
  useragent = "paperboy",
  save_dir = NULL,
  verbose = NULL,
  ...
)
}
\arguments{
\item{urls}{Character object with URLs.}

\item{collect_rss}{If one of the URLs contains an RSS feed, should it be
parsed.}

\item{timeout}{How long should the function wait for the connection (in
seconds). If the query finishes earlier, results are returned immediately.}

\item{ignore_fails}{normally the function errors when a URL can't be reached
due to connection issues. Setting to TRUE ignores this.}

\item{connections}{max total concurrent connections.}

\item{host_con}{max concurrent connections per host.}

\item{use_cookies}{If \code{TRUE}, use the \code{cookiemonster} package to
handle cookies. See \link[cookiemonster]{add_cookies} for details on how to
store cookies. Cookies are used to enter articles behind a paywall or
consent form.}

\item{useragent}{String to be sent in the User-Agent header.}

\item{save_dir}{store raw html data on disk instead of memory by providing a
path to a directory.}

\item{verbose}{A logical flag indicating whether information should be
printed to the screen. If \code{NULL} will be determined from
\code{getOption("paperboy_verbose")}.}

\item{...}{Currently not used}
}
\value{
A data.frame (tibble) with url status data and raw media text.
}
\description{
Collect data from supplied URLs
}
