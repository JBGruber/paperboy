[{"path":"https://jbgruber.github.io/paperboy/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Johannes B. Gruber. Author, maintainer. David Schoch. Contributor.","code":""},{"path":"https://jbgruber.github.io/paperboy/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Gruber J (2025). paperboy: Comprehensive Collection News Media Scrapers. R package version 0.0.7.9000, https://github.com/JBGruber/paperboy.","code":"@Manual{,   title = {paperboy: A Comprehensive Collection of News Media Scrapers},   author = {Johannes B. Gruber},   year = {2025},   note = {R package version 0.0.7.9000},   url = {https://github.com/JBGruber/paperboy}, }"},{"path":"https://jbgruber.github.io/paperboy/index.html","id":"paperboy-","dir":"","previous_headings":"","what":"A Comprehensive Collection of News Media Scrapers","title":"A Comprehensive Collection of News Media Scrapers","text":"philosophy paperboy package comprehensive collection webscraping scripts news media sites. Many data scientists researchers write code retrieve news media content websites. end research projects, code often collecting digital dust researchers hard drives instead made public others employ. paperboy offers writers webscraping scripts clear path publish code earn co-authorship package (see developers Section). users, promise simple: paperboy delivers news media data many websites consistent format. Check domains already supported table command pb_available().","code":""},{"path":"https://jbgruber.github.io/paperboy/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"A Comprehensive Collection of News Media Scrapers","text":"paperboy CRAN yet. Install via remotes (first install remotes via install.packages(\"remotes\"):","code":"remotes::install_github(\"JBGruber/paperboy\")"},{"path":"https://jbgruber.github.io/paperboy/index.html","id":"for-users","dir":"","previous_headings":"","what":"For Users","title":"A Comprehensive Collection of News Media Scrapers","text":"Say link news media article, example, mediacloud.org. Simply supply one multiple links media article main function, pb_deliver: returned data.frame contains important meta information news items full text. Notice, function problem reading link, even though shortened. paperboy unfinished highly experimental package moment. therefore often encounter warning: function still returns data.frame, important information missing — case isn’t . URLs processed normally though. dead link url vector, status column different 200 contain NAs. unhappy results generic approach, can still use second function package download raw html code later parse : pb_collect uses concurrent requests download many pages time, making function quick collect large amounts data. can experiment rvest another package extract information want df$content_raw.","code":"library(paperboy) df <- pb_deliver(\"https://tinyurl.com/386e98k5\") df pb_deliver(\"google.com\") #> ! No parser for domain google.com yet, attempting generic approach. pb_collect(\"google.com\")"},{"path":"https://jbgruber.github.io/paperboy/index.html","id":"for-developers","dir":"","previous_headings":"","what":"For developers","title":"A Comprehensive Collection of News Media Scrapers","text":"scraper news site want contribute one project, can become co-author package adding via pull request. First check available scrapers open issues pull requests. Open new issue comment existing one communicate working scraper (work isn’t done twice). start pulling articles pb_collect start parse html code content_raw column (preferably rvest). Every webscraper retrieve tibble following format: Since outlets give additional information, misc column included can retained.","code":""},{"path":"https://jbgruber.github.io/paperboy/index.html","id":"available-scrapers","dir":"","previous_headings":"","what":"Available Scrapers","title":"A Comprehensive Collection of News Media Scrapers","text":": Runs without known issues : Runs issues : Currently working, fix requested","code":""},{"path":"https://jbgruber.github.io/paperboy/reference/html_search.html","id":null,"dir":"Reference","previous_headings":"","what":"Search raw html for attributes — html_search","title":"Search raw html for attributes — html_search","text":"Search raw html attributes","code":""},{"path":"https://jbgruber.github.io/paperboy/reference/html_search.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Search raw html for attributes — html_search","text":"","code":"html_search(html, selectors, attributes = NULL, all = TRUE, n = 1L)"},{"path":"https://jbgruber.github.io/paperboy/reference/html_search.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Search raw html for attributes — html_search","text":"html raw html selectors vector CSS selectors include search. attributes attributes extract. NULL, returns text. TRUE, selectors collected. Otherwise, first non-empty result used. n multiple found, many return","code":""},{"path":"https://jbgruber.github.io/paperboy/reference/html_search.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Search raw html for attributes — html_search","text":"vector max length n","code":""},{"path":"https://jbgruber.github.io/paperboy/reference/pb_available.html","id":null,"dir":"Reference","previous_headings":"","what":"Show available parsers — pb_available","title":"Show available parsers — pb_available","text":"Show available parsers","code":""},{"path":"https://jbgruber.github.io/paperboy/reference/pb_available.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Show available parsers — pb_available","text":"","code":"pb_available(...)"},{"path":"https://jbgruber.github.io/paperboy/reference/pb_available.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Show available parsers — pb_available","text":"... optionally pass URLs check respective parser(s) /available.","code":""},{"path":"https://jbgruber.github.io/paperboy/reference/pb_available.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Show available parsers — pb_available","text":"character vector supported domains.","code":""},{"path":"https://jbgruber.github.io/paperboy/reference/pb_available.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Show available parsers — pb_available","text":"","code":"pb_available() #>   [1] \"3sat.de\"                        \"abendblatt.de\"                  #>   [3] \"abendzeitung.muenchen.de\"       \"ac24.cz\"                        #>   [5] \"ad.nl\"                          \"aktualne.cz\"                    #>   [7] \"anotherangryvoice.blogspot.com\" \"augsburger.allgemeine.de\"       #>   [9] \"badische.zeitung.de\"            \"bbc.co.uk\"                      #>  [11] \"berliner.kurier.de\"             \"berliner.zeitung.de\"            #>  [13] \"bild.de\"                        \"blesk.cz\"                       #>  [15] \"blogs.faz.net\"                  \"bnn.de\"                         #>  [17] \"br.de\"                          \"breakingnews.ie\"                #>  [19] \"breitbart.com\"                  \"businessinsider.de\"             #>  [21] \"buzzfeed.com\"                   \"cbsnews.com\"                    #>  [23] \"ceskatelevize.cz\"               \"cnet.com\"                       #>  [25] \"cnn.com\"                        \"dailymail.co.uk\"                #>  [27] \"decider.com\"                    \"democratandchronicle.com\"       #>  [29] \"denikn.cz\"                      \"denverpost.com\"                 #>  [31] \"der.postillon.com\"              \"derstandard.at\"                 #>  [33] \"derwesten.de\"                   \"deutschlandfunk.de\"             #>  [35] \"deutschlandfunkkultur.de\"       \"dnn.de\"                         #>  [37] \"echo24.de\"                      \"edition.cnn.com\"                #>  [39] \"epochtimes.de\"                  \"eu.courier.journal.com\"         #>  [41] \"eu.democratandchronicle.com\"    \"eu.tennessean.com\"              #>  [43] \"eu.usatoday.com\"                \"evolvepolitics.com\"             #>  [45] \"express.de\"                     \"faz.net\"                        #>  [47] \"finanzen.net\"                   \"fnp.de\"                         #>  [49] \"focus.de\"                       \"forbes.com\"                     #>  [51] \"foxbusiness.com\"                \"foxnews.com\"                    #>  [53] \"fr.de\"                          \"frankenpost.de\"                 #>  [55] \"freiepresse.de\"                 \"ftw.usatoday.com\"               #>  [57] \"geenstijl.nl\"                   \"golfweek.usatoday.com\"          #>  [59] \"handelsblatt.com\"               \"haz.de\"                         #>  [61] \"heidelberg24.de\"                \"heise.de\"                       #>  [63] \"hn.cz\"                          \"hna.de\"                         #>  [65] \"huffingtonpost.co.uk\"           \"huffingtonpost.com\"             #>  [67] \"huffpost.com\"                   \"idnes.cz\"                       #>  [69] \"independent.co.uk\"              \"independent.ie\"                 #>  [71] \"infranken.de\"                   \"irishexaminer.com\"              #>  [73] \"irishmirror.ie\"                 \"irishtimes.com\"                 #>  [75] \"irozhlas.cz\"                    \"joe.ie\"                         #>  [77] \"jungefreiheit.de\"               \"kabeleins.de\"                   #>  [79] \"karlsruhe.insider.de\"           \"kreiszeitung.de\"                #>  [81] \"ksta.de\"                        \"kurier.at\"                      #>  [83] \"latimes.com\"                    \"lidovky.cz\"                     #>  [85] \"lvz.de\"                         \"manager.magazin.de\"             #>  [87] \"marketwatch.com\"                \"maz.online.de\"                  #>  [89] \"mdr.de\"                         \"mediacourant.nl\"                #>  [91] \"merkur.de\"                      \"metronieuws.nl\"                 #>  [93] \"mmajunkie.usatoday.com\"         \"mopo.de\"                        #>  [95] \"morgenpost.de\"                  \"n.tv.de\"                        #>  [97] \"ndr.de\"                         \"news.de\"                        #>  [99] \"news.und.nachrichten.de\"        \"newsflash24.de\"                 #> [101] \"newstatesman.com\"               \"newsweek.com\"                   #> [103] \"nordkurier.de\"                  \"nos.nl\"                         #> [105] \"novinky.cz\"                     \"noz.de\"                         #> [107] \"nrc.nl\"                         \"nu.nl\"                          #> [109] \"nw.de\"                          \"nypost.com\"                     #> [111] \"nytimes.com\"                    \"nzz.ch\"                         #> [113] \"orf.at\"                         \"ostsee.zeitung.de\"              #> [115] \"pagesix.com\"                    \"parlamentnilisty.cz\"            #> [117] \"presseportal.de\"                \"prosieben.de\"                   #> [119] \"rbb24.de\"                       \"rnd.de\"                         #> [121] \"rollingstone.de\"                \"rp.online.de\"                   #> [123] \"rte.ie\"                         \"rtl.de\"                         #> [125] \"rtl.nl\"                         \"rtlnieuws.nl\"                   #> [127] \"ruhr24.de\"                      \"ruhrnachrichten.de\"             #> [129] \"saechsische.de\"                 \"schwaebische.de\"                #> [131] \"seznamzpravy.cz\"                \"sfgate.com\"                     #> [133] \"shz.de\"                         \"skwawkbox.org\"                  #> [135] \"sky.com\"                        \"spiegel.de\"                     #> [137] \"srf.ch\"                         \"stern.de\"                       #> [139] \"stuttgarter.zeitung.de\"         \"sueddeutsche.de\"                #> [141] \"suedkurier.de\"                  \"swp.de\"                         #> [143] \"swr3.de\"                        \"swr.de\"                         #> [145] \"swrfernsehen.de\"                \"t3n.de\"                         #> [147] \"t.online.de\"                    \"tag24.de\"                       #> [149] \"tagesschau.de\"                  \"tagesspiegel.de\"                #> [151] \"taz.de\"                         \"techrepublic.com\"               #> [153] \"telegraaf.nl\"                   \"telegraph.co.uk\"                #> [155] \"thecanary.co\"                   \"theguardian.com\"                #> [157] \"thejournal.ie\"                  \"thesun.ie\"                      #> [159] \"thueringer.allgemeine.de\"       \"tz.de\"                          #> [161] \"us.cnn.com\"                     \"usatoday.com\"                   #> [163] \"vice.com\"                       \"volkskrant.nl\"                  #> [165] \"volksstimme.de\"                 \"vox.de\"                         #> [167] \"wa.de\"                          \"washingtonpost.com\"             #> [169] \"watson.ch\"                      \"watson.de\"                      #> [171] \"waz.de\"                         \"wdr.de\"                         #> [173] \"welt.de\"                        \"wiwo.de\"                        #> [175] \"wsj.com\"                        \"wz.de\"                          #> [177] \"yahoo.com\"                      \"zdf.de\"                         #> [179] \"zeit.de\"                        pb_available(\"https://edition.cnn.com/\",              \"https://www.nytimes.com/\",              \"https://www.google.com/\") #> https://edition.cnn.com/ https://www.nytimes.com/  https://www.google.com/  #>                     TRUE                     TRUE                    FALSE"},{"path":"https://jbgruber.github.io/paperboy/reference/pb_collect.html","id":null,"dir":"Reference","previous_headings":"","what":"Collect data from supplied URLs — pb_collect","title":"Collect data from supplied URLs — pb_collect","text":"Collect data supplied URLs","code":""},{"path":"https://jbgruber.github.io/paperboy/reference/pb_collect.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Collect data from supplied URLs — pb_collect","text":"","code":"pb_collect(   urls,   collect_rss = TRUE,   timeout = 30,   ignore_fails = FALSE,   connections = 100L,   host_con = 6L,   use_cookies = FALSE,   useragent = \"paperboy\",   save_dir = NULL,   verbose = NULL,   ... )"},{"path":"https://jbgruber.github.io/paperboy/reference/pb_collect.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Collect data from supplied URLs — pb_collect","text":"urls Character object URLs. collect_rss one URLs contains RSS feed, parsed. timeout long function wait connection (seconds). query finishes earlier, results returned immediately. ignore_fails normally function errors URL reached due connection issues. Setting TRUE ignores . connections max total concurrent connections. host_con max concurrent connections per host. use_cookies TRUE, use cookiemonster package handle cookies. See add_cookies details store cookies. Cookies used enter articles behind paywall consent form. useragent String sent User-Agent header. save_dir store raw html data disk instead memory providing path directory. verbose logical flag indicating whether information printed screen. NULL determined getOption(\"paperboy_verbose\"). ... Currently used","code":""},{"path":"https://jbgruber.github.io/paperboy/reference/pb_collect.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Collect data from supplied URLs — pb_collect","text":"data.frame (tibble) url status data raw media text.","code":""},{"path":"https://jbgruber.github.io/paperboy/reference/pb_collect_rss.html","id":null,"dir":"Reference","previous_headings":"","what":"Collect RSS feed — pb_collect_rss","title":"Collect RSS feed — pb_collect_rss","text":"Collect articles RSS Atom feed(s)","code":""},{"path":"https://jbgruber.github.io/paperboy/reference/pb_collect_rss.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Collect RSS feed — pb_collect_rss","text":"","code":"pb_collect_rss(x, parse = TRUE, ...)"},{"path":"https://jbgruber.github.io/paperboy/reference/pb_collect_rss.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Collect RSS feed — pb_collect_rss","text":"x URL(s) RSS Atom feed(s). parse Whether results parsed data.frame. Turn debugging. ... passed pb_collect.","code":""},{"path":"https://jbgruber.github.io/paperboy/reference/pb_collect_rss.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Collect RSS feed — pb_collect_rss","text":"data.frame list","code":""},{"path":"https://jbgruber.github.io/paperboy/reference/pb_collect_rss.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Collect RSS feed — pb_collect_rss","text":"","code":"if (FALSE) { # \\dontrun{ pb_collect_rss(\"https://www.washingtonpost.com/arcio/rss/\") # works with atom feeds too pb_collect_rss(\"https://www.nu.nl/rss\") } # }"},{"path":"https://jbgruber.github.io/paperboy/reference/pb_deliver.html","id":null,"dir":"Reference","previous_headings":"","what":"Deliver online news articles — pb_deliver","title":"Deliver online news articles — pb_deliver","text":"function determine website urls given   call appropriate webscraper.","code":""},{"path":"https://jbgruber.github.io/paperboy/reference/pb_deliver.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Deliver online news articles — pb_deliver","text":"","code":"pb_deliver(x, try_default = TRUE, ignore_fails = FALSE, verbose = NULL, ...)"},{"path":"https://jbgruber.github.io/paperboy/reference/pb_deliver.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Deliver online news articles — pb_deliver","text":"x Either vector URLs data.frame returned pb_collect. try_default parser available, generic parser used TRUE URL skipped FALSE? ignore_fails normally function errors raw content URL parsed. Setting TRUE ignores parsing errors (use caution). verbose FALSE turns deliver silent. TRUE prints status messages progress bar screen. 2L turns debug mode. NULL determined getOption(\"paperboy_verbose\"). ... Passed pb_collect.","code":""},{"path":"https://jbgruber.github.io/paperboy/reference/pb_deliver.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Deliver online news articles — pb_deliver","text":"data.frame (tibble) media data full text.","code":""},{"path":"https://jbgruber.github.io/paperboy/reference/pb_deliver_paper.html","id":null,"dir":"Reference","previous_headings":"","what":"internal function to deliver specific newspapers — pb_deliver_paper","title":"internal function to deliver specific newspapers — pb_deliver_paper","text":"internal function deliver specific newspapers","code":""},{"path":"https://jbgruber.github.io/paperboy/reference/pb_deliver_paper.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"internal function to deliver specific newspapers — pb_deliver_paper","text":"","code":"pb_deliver_paper(x, verbose, pb, ...)"},{"path":"https://jbgruber.github.io/paperboy/reference/pb_deliver_paper.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"internal function to deliver specific newspapers — pb_deliver_paper","text":"x Either vector URLs data.frame returned pb_collect. verbose FALSE turns deliver silent. TRUE prints status messages progress bar screen. 2L turns debug mode. NULL determined getOption(\"paperboy_verbose\"). pb progress bar object. ... Passed pb_collect.","code":""},{"path":"https://jbgruber.github.io/paperboy/reference/pb_find_rss.html","id":null,"dir":"Reference","previous_headings":"","what":"Find RSS feed on a newspapers website — pb_find_rss","title":"Find RSS feed on a newspapers website — pb_find_rss","text":"Find RSS feed newspapers website","code":""},{"path":"https://jbgruber.github.io/paperboy/reference/pb_find_rss.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find RSS feed on a newspapers website — pb_find_rss","text":"","code":"pb_find_rss(x, use = c(\"main\", \"suffixes\", \"feedly\"))"},{"path":"https://jbgruber.github.io/paperboy/reference/pb_find_rss.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find RSS feed on a newspapers website — pb_find_rss","text":"x main domain newspaper site check RSS feeds. use steps include search (see Details). Default include .","code":""},{"path":"https://jbgruber.github.io/paperboy/reference/pb_find_rss.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Find RSS feed on a newspapers website — pb_find_rss","text":"URL RSS feed(s) NULL nothing found","code":""},{"path":"https://jbgruber.github.io/paperboy/reference/pb_find_rss.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Find RSS feed on a newspapers website — pb_find_rss","text":"Uses three step heuristic find RSS feeds: Scrapes main page (without paths) see RSS feed advertised Checks number common paths sites put RSS feeds Queries feedly.com API feeds associated page","code":""},{"path":"https://jbgruber.github.io/paperboy/reference/pb_find_rss.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Find RSS feed on a newspapers website — pb_find_rss","text":"Approach inspired https://github.com/mediacloud/feed_seeker","code":""},{"path":"https://jbgruber.github.io/paperboy/reference/pb_find_rss.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Find RSS feed on a newspapers website — pb_find_rss","text":"","code":"pb_find_rss(\"https://www.buzzfeed.com/\") #> ℹ Looking through links on the main page #> ✔ Looking through links on the main page [383ms] #>  #> ℹ Looking through common paths on the site #> ✔ Looking through common paths on the site [502ms] #>  #> ℹ Querying feedly API #> ✔ Querying feedly API [478ms] #>  #> ℹ Discovered 7 URLsCheck manually to see which ones fit #> # A tibble: 7 × 2 #>   source           url                                       #>   <chr>            <chr>                                     #> 1 landing page     https://www.buzzfeed.com/rss              #> 2 common locations https://buzzfeed.com/index.xml            #> 3 feedly API       https://www.buzzfeed.com/index            #> 4 feedly API       https://www.buzzfeed.com/food             #> 5 feedly API       https://www.buzzfeed.com/badge/omg        #> 6 feedly API       https://www.buzzfeed.com/celebrity        #> 7 feedly API       https://www.buzzfeed.com/badge/collection"},{"path":"https://jbgruber.github.io/paperboy/reference/pb_inspect.html","id":null,"dir":"Reference","previous_headings":"","what":"Inspect content collected with pb_collect — pb_inspect","title":"Inspect content collected with pb_collect — pb_inspect","text":"Opens browser display content saved row data.frame created pb_collect.","code":""},{"path":"https://jbgruber.github.io/paperboy/reference/pb_inspect.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Inspect content collected with pb_collect — pb_inspect","text":"","code":"pb_inspect(x, i = 1L, host_ip = \"127.0.0.1\", port = httpuv::randomPort())"},{"path":"https://jbgruber.github.io/paperboy/reference/pb_inspect.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Inspect content collected with pb_collect — pb_inspect","text":"x data.frame returned pb_collect. entry display. host_ip, port host IP port create temporary web server shows content.","code":""},{"path":"https://jbgruber.github.io/paperboy/reference/pb_new.html","id":null,"dir":"Reference","previous_headings":"","what":"Create new scraper — pb_new","title":"Create new scraper — pb_new","text":"Create new scraper","code":""},{"path":"https://jbgruber.github.io/paperboy/reference/pb_new.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create new scraper — pb_new","text":"","code":"pb_new(np, author = \"\", issue = \"\")"},{"path":"https://jbgruber.github.io/paperboy/reference/pb_new.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create new scraper — pb_new","text":"np domain URL newspaper scraper . author wrote . issue GitHub issue?","code":""},{"path":"https://jbgruber.github.io/paperboy/reference/pb_new.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create new scraper — pb_new","text":"","code":"if (FALSE) { # \\dontrun{ paperboy:::pb_new(np = \"https://www.buzzfeed.com/\",                   author = \"[@JBGruber](https://github.com/JBGruber/)\")  paperboy:::pb_new_done() } # }"},{"path":"https://jbgruber.github.io/paperboy/reference/pb_read_cookies.html","id":null,"dir":"Reference","previous_headings":"","what":"Read in cookie file — pb_read_cookies","title":"Read in cookie file — pb_read_cookies","text":"Deprecated favour add_cookies.","code":""},{"path":"https://jbgruber.github.io/paperboy/reference/pb_read_cookies.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Read in cookie file — pb_read_cookies","text":"","code":"pb_read_cookies(...)"},{"path":"https://jbgruber.github.io/paperboy/reference/pb_read_cookies.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Read in cookie file — pb_read_cookies","text":"... used.","code":""},{"path":"https://jbgruber.github.io/paperboy/reference/reexports.html","id":null,"dir":"Reference","previous_headings":"","what":"Objects exported from other packages — reexports","title":"Objects exported from other packages — reexports","text":"objects imported packages. Follow links see documentation. magrittr %>%","code":""},{"path":"https://jbgruber.github.io/paperboy/reference/test_parser.html","id":null,"dir":"Reference","previous_headings":"","what":"Test a Parser — test_parser","title":"Test a Parser — test_parser","text":"Test parser using data frame pb_collect.","code":""},{"path":"https://jbgruber.github.io/paperboy/reference/test_parser.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test a Parser — test_parser","text":"","code":"test_parser(test_data)"},{"path":"https://jbgruber.github.io/paperboy/reference/test_parser.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test a Parser — test_parser","text":"test_data data frame raw content.","code":""},{"path":"https://jbgruber.github.io/paperboy/reference/test_parser.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test a Parser — test_parser","text":"success failure message.","code":""},{"path":"https://jbgruber.github.io/paperboy/reference/use_new_parser.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a new parser — use_new_parser","title":"Create a new parser — use_new_parser","text":"Create new parser given domain.","code":""},{"path":"https://jbgruber.github.io/paperboy/reference/use_new_parser.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a new parser — use_new_parser","text":"","code":"use_new_parser(x, author = \"\", issue = \"\", rss = NULL, test_data = NULL)"},{"path":"https://jbgruber.github.io/paperboy/reference/use_new_parser.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a new parser — use_new_parser","text":"x character string URL newspaper create parser . author Markdown formatted character string author parser. issue Markdown formatted link issue associated parser (please file issue starting work new parser). rss optional character string RSS feed associated parser. test_data optional data frame test data use testing parser.","code":""},{"path":"https://jbgruber.github.io/paperboy/reference/use_new_parser.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a new parser — use_new_parser","text":"message indicating success failure parser creation.","code":""},{"path":"https://jbgruber.github.io/paperboy/reference/use_new_parser.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create a new parser — use_new_parser","text":"function process steps creating new parser, : Creating file parser template Trying find RSS feed link Add new entry status.csv file, contains information parsers. Edit parsers extract required additional information articles site. Check parser consistency (can loaded? entries present?). Check test data (either provided function downloaded RSS feed). Finalise CSV entry might obvious, steps can performed single action. Rather idea run function multiple times, done.","code":""},{"path":"https://jbgruber.github.io/paperboy/reference/use_new_parser.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a new parser — use_new_parser","text":"","code":"if (FALSE) { # \\dontrun{ use_new_parser(x = \"https://www.buzzfeed.com/\",                author = \"[@JBGruber](https://github.com/JBGruber/)\",                issue = \"[#1](https://github.com/JBGruber/paperboy/issues/1)\",                rss = \"https://www.buzzfeed.com/rss\") } # }"}]
